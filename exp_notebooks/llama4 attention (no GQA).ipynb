{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3030d383-7142-4ed3-b9f3-4ebabf287b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7edf87fc-dd19-4c0c-a91c-795a4c713904",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 1: Setup the configurations for the different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6780dc29-c366-44a8-8550-cdd2163911a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 10\n",
    "num_attention_heads = 2\n",
    "qkv_vec_dim = 8\n",
    "\n",
    "head_dim = qkv_vec_dim // num_attention_heads\n",
    "assert head_dim % 2 == 0, 'Head dimension must be an even number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfb2f14-526e-441f-ad70-20f243331f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128\n",
    "rope_theta = 10000.0 # Base angle for RoPE frequency calculation\n",
    "rms_norm_eps = 1e-5 # Epsilon for RMSNorm\n",
    "using_attention_bias = False # Whether to use bias in Q, K, V, O\n",
    "normalising_qk = True # Whether to apply L2 normalisaition to Q & K before attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb449c29-4aac-4912-9655-1860d1d6674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 2: Get the vector embeddings & position IDs of each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709137ff-30aa-4bfb-971d-30058bacbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Input\n",
    "batch_size = 2\n",
    "sequence_length = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae72008-4a28-4fb8-827a-3b35151bff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix representing the embedding vectors for each token in the sequence\n",
    "embeddings = torch.randn(batch_size, sequence_length, emb_size)\n",
    "    # Shape: (batch_size, sequence_length, emb_size)\n",
    "    # ShapeEx: (2, 9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b73a6f-fa29-4ef9-9000-e979ba3b0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "    # Shape: (batch_size, sequence_length)\n",
    "    # ShapeEx: (2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe1ebe75-231f-4d13-8d14-4dca4226e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "print(position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beee6c96-32f5-4b47-8ee3-cdb2257e2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 3: Create the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef3d44e-b04e-48b7-b446-8d3c80826766",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -torch.inf, diagonal=1)\n",
    "    # torch.triu() keeps the above main diagonal, while setting main diagonal & below to 0\n",
    "    # Shape: (sequence_length, sequence_length)\n",
    "    # ShapeEx: (9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d506944-7c7b-4033-9af4-c78c77c466f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d63fea6-6d26-4db7-aee2-e040fe2e4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0)\n",
    "    # Shape: (1, 1, sequence_length, sequence_length)\n",
    "    # ShapeEx: (1, 1, 9, 9)\n",
    "attention_mask = attention_mask.expand(batch_size, 1, -1, -1)\n",
    "    # Shape: (2, 1, sequence_length, sequence_length)\n",
    "    # ShapeEx: (2, 1, 9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a36b511d-bcc0-4b65-8f5c-e1f064027e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 4: Create Q, K, V, O projection matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "934cbc81-94de-4280-83cd-a383621775c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Query of each token in the sequence\n",
    "# K: Key of each token in the sequence\n",
    "# V: Value or information of each token in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5199e2f5-50b6-4f19-a5ec-b9f190f434bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_proj = nn.Linear(emb_size, qkv_vec_dim, bias=using_attention_bias)\n",
    "k_proj = nn.Linear(emb_size, qkv_vec_dim, bias=using_attention_bias)\n",
    "v_proj = nn.Linear(emb_size, qkv_vec_dim, bias=using_attention_bias)\n",
    "    # Shape: (emb_size, qkv_vec_dim)\n",
    "    # ShapeEx: (10, 8)\n",
    "\n",
    "o_proj = nn.Linear(qkv_vec_dim, emb_size, bias=using_attention_bias)\n",
    "    # Since its a bit like reversal of v_proj, its shape is also reversed\n",
    "    # Shape: (qkv_vec_dim, emb_size)\n",
    "    # ShapeEx: (8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "932351c7-d811-4d58-b497-dbac35934113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=8, out_features=4, bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(q_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "407ccfbd-bba6-46fc-afc1-4c0bf84c6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 5: Compute Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02e4448f-69b4-4921-87d6-f95323df7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = q_proj(embeddings)\n",
    "key_states = k_proj(embeddings)\n",
    "value_states = v_proj(embeddings)\n",
    "    # Shape: (batch_size, sequence_length, qkv_vec_dim)\n",
    "    # ShapeEx: (2, 9, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99d037a6-2d33-492f-a59c-9a463fc11f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = query_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(batch_size, sequence_length, num_attention_heads, head_dim).transpose(1, 2)\n",
    "    # Shape: (batch_size, num_attention_heads, sequence_length, head_dim)\n",
    "    # ShapeEx: (2, 2, 9, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe8b7d4a-e244-4173-894a-5fc87f6b0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 6: Compute RoPE complex frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9cae0ed-9985-4e59-aa93-adaec10448ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotary Positional Embeddings (RoPE) applies rotations to the Q & K vectors based on the position of the tokens,\n",
    "# injecting relative positional information into Q & K before computing their dot product.\n",
    "\n",
    "# RoPE represents embeddings in complex number space and rotates the embeddings by an angle proportional to the tokens' position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "643b5ee0-d67f-4de8-9266-15b0024607d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rope_freqs_cis(emb_dim, max_seq_len, base=10000.0, device=None):\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, emb_dim, 2, device=device).float() / emb_dim))\n",
    "        # Shape: (head_dim // 2, )\n",
    "        # ShapeEx: (2, )\n",
    "    pos_indices = torch.arange(max_seq_len, device=device).type_as(inv_freq)\n",
    "        # Shape: (max_seq_len, )\n",
    "        # ShapeEx: (128, )\n",
    "    freqs = torch.outer(pos_indices, inv_freq)\n",
    "        # Shape: (max_seq_len, head_dim // 2)\n",
    "        # ShapeEx: (128, 2)\n",
    "\n",
    "    freqs_cis = torch.complex(freqs.cos(), freqs.sin())\n",
    "        # Shape: (max_seq_len, head_dim // 2), complex \n",
    "        # ShapeEx: (128, 2), complex\n",
    "\n",
    "    # freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bec1e-35a1-43f5-8eb0-ebd812257ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope_to_qk(\n",
    "    q_vecs: torch.Tensor,    # Shape: (batch, num_attention_heads, sequence_length, head_dim); ShapeEx: (2, 2, 9, 4)\n",
    "    k_vecs: torch.Tensor,    # Shape: (batch, num_attention_heads, sequence_length, head_dim): ShapeEx: (2, 2, 9, 4)\n",
    "    freqs_cis: torch.Tensor  # Shape: (max_seq_len, head_dim // 2), complex\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # 1. Select the correct rotation vectors based on the current sequence positions (truncate)\n",
    "    #    position_ids has shape of (batch, sequence_length)\n",
    "    #    This uses advanced indexing to pick rows from freqs_cis based on the position_ids\n",
    "    freqs_cis = freqs_cis[position_ids]\n",
    "        # Shape: (batch, sequence_length, head_dim // 2), complex\n",
    "        # ShapeEx: (2, 9, 2), complex\n",
    "\n",
    "    # 2. Add an extra dimension for broadcasting across attention heads\n",
    "    freqs_cis = freqs_cis[:, None, :, :]\n",
    "        # Shape: (batch, 1, sequence_length, head_dim // 2), complex\n",
    "        # ShapeEx: (1, 1, 9, 2), complex\n",
    "\n",
    "    # 3. Reshape Q & K to view adjacent pairs as complex numbers\n",
    "        # q_vecs Shape: (batch, num_attention_heads, sequence_length, head_dim)\n",
    "        #   -> (batch, num_attention_heads, sequence_length, head_dim // 2, 2)\n",
    "        #   -> (batch, num_attention_heads, sequence_length, head_dim // 2), complex\n",
    "        # ShapeEx: (2, 2, 9, 4) -> (2, 2, 9, 2, 2) -> (2, 2, 9, 2)\n",
    "    q_vecs_complex = torch.view_as_complex(q_vecs.float().reshape(*q_vecs.shape[:-1], -1, 2))\n",
    "    k_vecs_complex = torch.view_as_complex(k_vecs.float().reshape(*k_vecs.shape[:-1], -1, 2))\n",
    "\n",
    "    # 4. Apply the RoPE rotation to Q & K using element-wise complex multiplication\n",
    "    # q_vecs_complex Shape: (batch, num_attention_heads, sequence_length, head_dim // 2), complex\n",
    "        # ShapeEx: (2, 2, 9, 2)\n",
    "    # freqs_cis Shape: (batch, 1, sequence_length, head_dim // 2), complex\n",
    "        # ShapeEx: (2, 1, 9, 2)\n",
    "    rotated_q_vecs_complex = q_vecs_complex * freqs_cis\n",
    "    rotated_k_vecs_complex = k_vecs_complex * freqs_cis\n",
    "\n",
    "    # 5. Covert Q & K from complex numbers back to real numbers. Basically reversal of Step (3)\n",
    "        # rotated_q_vecs_complex Shape: (batch, num_attention_heads, sequence_length, head_dim // 2), complex\n",
    "        #   -> (batch, num_attention_heads, sequence_length, head_dim // 2, 2)\n",
    "        #   -> (batch, num_attention_heads, sequence_length, head_dim)\n",
    "        # ShapeEx: (2, 2, 9, 2) -> (2, 2, 9, 2, 2) -> (2, 2, 9, 4)\n",
    "    rotated_q_vecs_real = torch.view_as_real(rotated_q_vecs_complex).flatten(3)\n",
    "    rotated_k_vecs_real = torch.view_as_real(rotated_k_vecs_complex).flatten(3)\n",
    "    \n",
    "    return rotated_q_vecs_real.type_as(q_vecs), rotated_k_vecs_real.type_as(k_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b7b85-a435-4225-a06c-7cd7e1ed9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_cis = simple_rope_calculation(head_dim, max_seq_len, base=rope_theta, device=embeddings.device)\n",
    "    # Shape: (max_seq_len, head_dim // 2), complex \n",
    "    # ShapeEx: (128, 2)\n",
    "\n",
    "query_states_rope, key_states_rope = apply_rope_to_qk(query_states, key_states, freqs__cis)\n",
    "    # Shape: (batch, num_attention_heads, sequence_length, head_dim)\n",
    "    # ShapeEx: (2, 2, 9, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ae320c8-737d-4b99-bb74-ad67d156b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional L2 Normalization\n",
    "# To be applied to Q & K after RoPE but before the attenton score calculation (the matrix multiplication)\n",
    "class SimpleL2Norm(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # Epsilon value to avoid division by zero during normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c02cb-66bc-4bc0-a24a-6bbe4658d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalising_qk:\n",
    "    qk_norm = SimpleL2Norm()\n",
    "    query_states_final = qk_norm(query_states_rope)\n",
    "    key_states_final = qk_norm(key_states_rope)\n",
    "else:\n",
    "    query_states_final = query_states_rope\n",
    "    key_states_final = key_states_rope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e535e0-570a-472a-a6a6-dd45bb301709",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.matmul(query_states_final, key_states_final)\n",
    "\n",
    "scaling_factor = 1.0 / math.sqrt(head_dim)\n",
    "attn_weights *= scaling_factor\n",
    "attn_weights = attn_weights + attention_mask\n",
    "attn_weights = nn.functional.softmax(attn_weights, dim=-1).to(query_states.dtype)\n",
    "    # Shape: (batch_size, num_attn_heads, sequence_length, sequence_length)\n",
    "    # ShapeEx: (2, 2, 9, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf6a06-70ec-459e-a6ac-53e37b27de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = torch.matmul(attn_weights, value_states)\n",
    "    # Shape: (batch_size, num_attn_heads, sequence_length, head_dim)\n",
    "    # ShapeEx: (2, 2, 9, 8)\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.view(batch_size, sequence_length, hidden_size)\n",
    "# Shape: (batch_size, num_attn_heads, sequence_length, head_dim)\n",
    "# -> (batch_size, sequence_length, num_attn_heads, head_dim)\n",
    "# -> (batch, seq_len, num_attn_heads * head_dim) = (batch, seq_len, qkv_vec_dim)\n",
    "final_attn_output = o_project(attn_output)\n",
    "    # Shape: (batch, seq_len, emb_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
